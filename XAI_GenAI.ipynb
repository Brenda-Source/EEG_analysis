{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Responsible AI: XAI GenAI project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is finalised by Mabel-Brenda Ifeoma Okikaa, s184075.\n",
    "The first part of my assignment was dicussed with my group. Therefore, some similarites may occur. \n",
    "The group consists of: Kjær, Christian Valentin. Kolbert, Jedrzej Konrad & Madsen, Andreas Råskov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Background\n",
    "\n",
    "\n",
    "\n",
    "Based on the previous lessons on explainability, post-hoc methods are used to explain the model, such as saliency map, SmoothGrad, LRP, LIME, and SHAP. Take LRP (Layer Wise Relevance Propagation) as an example; it highlights the most relevant pixels to obtain a prediction of the class \"cat\" by backpropagating the relevance. (image source: [Montavon et. al (2016)](https://giorgiomorales.github.io/Layer-wise-Relevance-Propagation-in-Pytorch/))\n",
    "\n",
    "<!-- %%[markdown] -->\n",
    "![LRP example](images/catLRP.jpg)\n",
    "\n",
    "Another example is about text sentiment classification, here we show a case of visualizing the importance of words given the prediction of 'positive':\n",
    "\n",
    "![text example](images/textGradL2.png)\n",
    "\n",
    "where the words highlight with darker colours indicate to be more critical in predicting the sentence to be 'positive' in sentiment.\n",
    "More examples could be found [here](http://34.160.227.66/?models=sst2-tiny&dataset=sst_dev&hidden_modules=Explanations_Attention&layout=default).\n",
    "\n",
    "Both cases above require the class or the prediction of the model. But:\n",
    "\n",
    "***How do you explain a model that does not predict but generates?***\n",
    "\n",
    "In this project, we will work on explaining the generative model based on the dependency between words. We will first look at a simple example, and using Point-wise Mutual Information (PMI) to compute the saliency map of the sentence. After that we will contruct the expereiment step by step, followed by exercises and questions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. A simple example to start with\n",
    "Given a sample sentence: \n",
    "> *Tokyo is the capital city of Japan.* \n",
    "\n",
    "We are going to explain this sentence by finding the dependency using a saliency map between words.\n",
    "The dependency of two words in the sentence could be measured by [Point-wise mutual information (PMI)](https://en.wikipedia.org/wiki/Pointwise_mutual_information): \n",
    "\n",
    "\n",
    "Mask two words out, e.g. \n",
    "> \\[MASK-1\\] is the captial city of \\[MASK-2\\].\n",
    "\n",
    "\n",
    "Ask the generative model to fill in the sentence 10 times, and we have:\n",
    "\n",
    "| MASK-1      | MASK-2 |\n",
    "| ----------- | ----------- |\n",
    "|    tokyo   |     japan   |\n",
    "|  paris  |     france    |\n",
    "|  london  |     england    |\n",
    "|  paris  |     france    |\n",
    "|  beijing |  china |\n",
    "|    tokyo   |     japan   |\n",
    "|  paris  |     france    |\n",
    "|  paris  |     france    |\n",
    "|  london  |     england    |\n",
    "|  beijing |  china |\n",
    "\n",
    "PMI is calculated by: \n",
    "\n",
    "$PMI(x,y)=log_2⁡ \\frac{p(\\{x,y\\}| s-\\{x,y\\})}{P(\\{x\\}|s-\\{x,y\\})P(\\{y\\}|s-\\{x,y\\})}$\n",
    "\n",
    "where $x$, $y$ represents the words that we masked out, $s$ represents the setence, and $s-\\{x,y\\}$ represents the sentences tokens after removing the words $x$ and $y$.\n",
    "\n",
    "In this example we have $PMI(Tokyo, capital) = log_2 \\frac{0.2}{0.2 * 0.2} = 2.32$\n",
    "\n",
    "Select an interesting word in the sentences; we can now compute the PMI between all other words and the chosen word using the generative model:\n",
    "(Here, we use a longer sentence and run 20 responses per word.)\n",
    "![](images/resPMI.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparation\n",
    "### 2.1 Conda enviroment\n",
    "\n",
    "```\n",
    "conda env create -f environment.yml\n",
    "conda activate xai_llm\n",
    "```\n",
    "\n",
    "\n",
    "### 2.2 Download the offline LLM\n",
    "\n",
    "We use the offline LLM model from hugging face. It's approximately 5 GB.\n",
    "Download it using the comman below, and save it under `./models/`.\n",
    "```\n",
    "huggingface-cli download TheBloke/openchat-3.5-0106-GGUF openchat-3.5-0106.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False\n",
    "# credit to https://huggingface.co/TheBloke/openchat-3.5-0106-GGUF\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mask the sentence and get the responses from LLM\n",
    "### 3.1 Get the input sentence\n",
    "\n",
    "**Remember to change the anchor word index when changing the input sentence.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  Tokyo is the capital city of Japan\n"
     ]
    }
   ],
   "source": [
    "def get_input():\n",
    "    # ideally this reads inputs from a file, now it just takes an input\n",
    "    return input(\"Enter a sentence: \")\n",
    "    \n",
    "anchor_word_idx = 0 # the index of the interested word\n",
    "prompts_per_word = 20 # number of generated responses  \n",
    "\n",
    "sentence = get_input()\n",
    "print(\"Sentence: \", sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from ./models/openchat-3.5-0106.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = openchat_openchat-3.5-0106\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32002]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32002]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32002]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 261/32002 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32002\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = openchat_openchat-3.5-0106\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 32000 '<|end_of_turn|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4165.38 MiB\n",
      "................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 160\n",
      "llama_new_context_with_model: n_batch    = 150\n",
      "llama_new_context_with_model: n_ubatch   = 150\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    20.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   20.00 MiB, K (f16):   10.00 MiB, V (f16):   10.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    23.53 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: openchat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.name': 'openchat_openchat-3.5-0106', 'general.architecture': 'llama', 'llama.context_length': '8192', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '32000', 'general.file_type': '15', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{{ 'GPT4 Correct ' + message['role'].title() + ': ' + message['content'] + '<|end_of_turn|>'}}{% endfor %}{% if add_generation_prompt %}{{ 'GPT4 Correct Assistant:' }}{% endif %}\"}\n",
      "Using gguf chat template: {{ bos_token }}{% for message in messages %}{{ 'GPT4 Correct ' + message['role'].title() + ': ' + message['content'] + '<|end_of_turn|>'}}{% endfor %}{% if add_generation_prompt %}{{ 'GPT4 Correct Assistant:' }}{% endif %}\n",
      "Using chat eos_token: <|end_of_turn|>\n",
      "Using chat bos_token: <s>\n"
     ]
    }
   ],
   "source": [
    "from models.ChatModel import ChatModel\n",
    "model_name = \"openchat\"\n",
    "model = ChatModel(model_name)\n",
    "print(f\"Model: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Run the prompts and get all the responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.command_generator import generate_prompts, prefix_prompt\n",
    "from tools.evaluate_response import get_replacements\n",
    "from tqdm import tqdm\n",
    "\n",
    "def run_prompts(model, sentence, anchor_idx, prompts_per_word=20):\n",
    "    prompts = generate_prompts(sentence, anchor_idx)\n",
    "    all_replacements = []\n",
    "    for prompt in prompts:\n",
    "        replacements = []\n",
    "        for _ in tqdm(\n",
    "            range(prompts_per_word),\n",
    "            desc=f\"Input: {prompt}\",\n",
    "        ):\n",
    "            response = model.get_response(\n",
    "                prefix_prompt(prompt),\n",
    "            ).strip()\n",
    "            if response:\n",
    "                replacement = get_replacements(prompt, response)\n",
    "                if replacement:\n",
    "                    replacements.append(replacement)\n",
    "        if len(replacements) > 0:\n",
    "            all_replacements.append(replacements)\n",
    "    return all_replacements\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] [MASK] the capital city of Japan:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] [MASK] the capital city of Japan:  50%|█████     | 10/20 [00:26<00:17,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', '[mask]', 'the', 'capital', 'city', 'of', 'japan'] ['tokyo']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] [MASK] the capital city of Japan: 100%|██████████| 20/20 [00:49<00:00,  2.47s/it]\n",
      "Input: [MASK] is [MASK] capital city of Japan:  70%|███████   | 14/20 [00:32<00:13,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'is', '[mask]', 'capital', 'city', 'of', 'japan'] ['tokyo', 'is', 'japans', 'capital', 'city']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] is [MASK] capital city of Japan:  90%|█████████ | 18/20 [00:40<00:03,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'is', '[mask]', 'capital', 'city', 'of', 'japan'] ['tokyo', 'is', '[mask]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] is [MASK] capital city of Japan: 100%|██████████| 20/20 [00:46<00:00,  2.31s/it]\n",
      "Input: [MASK] is the [MASK] city of Japan:  90%|█████████ | 18/20 [00:48<00:04,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'is', 'the', '[mask]', 'city', 'of', 'japan'] ['kyoto', 'is', 'the', 'ancient', 'capital', 'of', 'japan']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] is the [MASK] city of Japan: 100%|██████████| 20/20 [00:52<00:00,  2.63s/it]\n",
      "Input: [MASK] is the capital [MASK] of Japan:  65%|██████▌   | 13/20 [00:30<00:16,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'is', 'the', 'capital', '[mask]', 'of', 'japan'] ['tokyo', 'is', 'the', 'capital', '[japan]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] is the capital [MASK] of Japan:  80%|████████  | 16/20 [00:36<00:08,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'is', 'the', 'capital', '[mask]', 'of', 'japan'] ['tokyo', 'is', 'the', 'capital', 'tokyo']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] is the capital [MASK] of Japan: 100%|██████████| 20/20 [00:53<00:00,  2.68s/it]\n",
      "Input: [MASK] is the capital city [MASK] Japan: 100%|██████████| 20/20 [01:02<00:00,  3.14s/it]\n",
      "Input: [MASK] is the capital city of [MASK]: 100%|██████████| 20/20 [00:47<00:00,  2.38s/it]\n"
     ]
    }
   ],
   "source": [
    "# Example A: Tokyo is the capital city of Japan.\n",
    "all_responses = run_prompts(model, sentence, anchor_word_idx, prompts_per_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['tokyo', 'is'], ['tokyo', 'is'], ['tokyo', 'is'], ['tokyo', 'is'], ['tokyo', 'is'], ['tokyo', 'is'], ['tokyo', 'is'], ['tokyo', 'is'], ['tokyo', 'is'], ['', ''], ['tokyo', 'is'], ['tokyo', 'is'], ['tokyo', 'is'], ['tokyo', 'is'], ['tokyo', 'is'], ['tokyo', 'is'], ['tokyo', 'is'], ['tokyo', 'is'], ['tokyo', 'is'], ['tokyo', 'is']]\n",
      "[['tokyo', 'tokyos'], ['tokyo', 'the'], ['tokyo', 'the'], ['tokyo', 'tokyos'], ['tokyo', 'tokyos'], ['tokyo', 'the'], ['tokyo', 'the'], ['tokyo', 'tokyos'], ['tokyo', 'the'], ['tokyo', 'the'], ['tokyo', 'the'], ['tokyo', '[mask]'], ['tokyo', 'the'], ['', ''], ['tokyo', 'tokyos'], ['tokyo', 'the'], ['tokyo', 'the'], ['', ''], ['tokyo', 'tokyos'], ['tokyo', '[tokyos]']]\n",
      "[['tokyo', 'capital'], ['kyoto', 'ancient'], ['tokyo', 'capital'], ['tokyo', 'capital'], ['kyoto', 'ancient'], ['tokyo', 'capital'], ['tokyo', 'capital'], ['kyoto', 'former [mask]'], ['osaka', 'second largest'], ['tokyo', 'capital'], ['hiroshima', 'second largest'], ['tokyo', 'bustling capital'], ['hiroshima', 'ancient'], ['tokyo', 'largest'], ['kyoto', 'historic'], ['kyoto', 'ancient'], ['tokyo', 'largest'], ['', ''], ['tokyo', 'bustling capital'], ['tokyo', 'capital']]\n",
      "[['tokyo', 'city'], ['tokyo', 'city'], ['tokyo', 'city'], ['tokyo', ''], ['tokyo', ''], ['tokyo', ''], ['tokyo', 'tokyo'], ['tokyo', 'city'], ['tokyo', ''], ['tokyo', 'city'], ['tokyo', 'tokyo'], ['tokyo', 'city'], ['', ''], ['tokyo', ''], ['tokyo', ''], ['', ''], ['tokyo', 'city'], ['tokyo', 'city'], ['tokyo', ''], ['tokyo', 'city']]\n",
      "[['tokyo', 'of'], ['tokyo', 'of'], ['tokyo', 'of'], ['tokyo', 'of'], ['tokyo', 'of'], ['tokyo', 'of'], ['tokyo', 'of'], ['tokyo', 'of'], ['tokyo', 'of'], ['tokyo', 'of'], ['tokyo', 'of'], ['tokyo', 'of'], ['tokyo', 'of'], ['tokyo', 'of'], ['tokyo', 'of'], ['tokyo', 'of'], ['tokyo', 'of'], ['tokyo', 'of'], ['tokyo', 'of'], ['tokyo', 'of']]\n",
      "[['tokyo', 'japan'], ['new delhi', 'india'], ['paris', 'france'], ['tokyo', 'japan'], ['london', 'england'], ['london', 'england'], ['london', 'the united kingdom'], ['paris', 'france'], ['ankara', 'turkey'], ['paris', 'france'], ['london', 'the united kingdom'], ['madrid', 'spain'], ['tokyo', 'japan'], ['paris', 'france'], ['london', 'england'], ['tokyo', 'japan'], ['paris', 'france'], ['tokyo', 'japan'], ['paris', 'france'], ['paris', 'france']]\n"
     ]
    }
   ],
   "source": [
    "# We print out the computed replacements for the masks\n",
    "for each_output in all_responses:\n",
    "    print(each_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 EXERCISE: compute the PMI for each word\n",
    "\n",
    "$PMI(x,y)=log_2⁡ \\frac{p(\\{x,y\\}| s-\\{x,y\\})}{P(\\{x\\}|s-\\{x,y\\})P(\\{y\\}|s-\\{x,y\\})}$\n",
    "\n",
    "* Compute the $P(x)$, $P(y)$ and $P(x,y)$ first and print it out.\n",
    "* Compute the PMI for each word.\n",
    "* Visualize the result by coloring. Tips: you might need to normalize the result first. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\s184075\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "nltk.download('punkt')\n",
    "import numpy as np\n",
    "\n",
    "def compute_pmi(sentence, all_responses, anchor_word_idx, prompts_per_word):\n",
    "    # Tokenize the sentence (splits )\n",
    "    words = word_tokenize(sentence.lower())\n",
    "\n",
    "    # Clean punctuation and so forth \n",
    "    tokens = [token for token in words if token.isalnum()]\n",
    "    \n",
    "    # Convert words to lowercase\n",
    "    words = [word.lower() for word in tokens]\n",
    "\n",
    "    # Create a DataFrame to store PMI calculations\n",
    "    pmi_df = pd.DataFrame(columns=words)\n",
    "    pmi_df.loc['px'] = 0\n",
    "    pmi_df.loc['py'] = 0\n",
    "    pmi_df.loc['pxy'] = 0\n",
    "    pmi_df.loc['pmi'] = 0\n",
    "    pmi_df.loc['saliency'] = 0\n",
    "\n",
    "    # variables\n",
    "    idx_y = 0\n",
    "    word_x = words[anchor_word_idx].lower()\n",
    "\n",
    "    epsi = 1e-10 # no division by zero \n",
    "    # Iterate over each response\n",
    "    for i, responses in enumerate(all_responses):\n",
    "        px = epsi\n",
    "        py = epsi\n",
    "        pxy = epsi\n",
    "\n",
    "        # Check response index matches the anchor word index\n",
    "        if anchor_word_idx == i:\n",
    "            idx_y = 1\n",
    "\n",
    "        word_y = words[i + idx_y].lower()\n",
    "\n",
    "        # Iterate over each response pair\n",
    "        for response in responses:\n",
    "            x = response[0]\n",
    "            y = response[1]\n",
    "            if x == word_x:\n",
    "                px += 1\n",
    "            if y == word_y:\n",
    "                py += 1\n",
    "            if x == word_x and y == word_y:\n",
    "                pxy += 1\n",
    "\n",
    "        # Calculate probabilities and PMI\n",
    "        px = px / prompts_per_word\n",
    "        py = py / prompts_per_word\n",
    "        pxy = pxy / prompts_per_word\n",
    "        pmi = np.log2(pxy / (px * py))\n",
    "\n",
    "        # Update DataFrame with calculated values\n",
    "        pmi_df.at['px', word_y] = px\n",
    "        pmi_df.at['py', word_y] = py\n",
    "        pmi_df.at['pxy', word_y] = pxy\n",
    "        pmi_df.at['pmi', word_y] = pmi\n",
    "\n",
    "    # Set the anchor word column to NaN\n",
    "    pmi_df[word_x] = np.nan\n",
    "\n",
    "    # Normalize PMI values and calculate saliency\n",
    "    min_pmi = np.round(pmi_df.loc['pmi'].min(), 10)\n",
    "    max_pmi = pmi_df.loc['pmi'].max()\n",
    "    pmi_df.loc['saliency'] = (pmi_df.loc['pmi'] - min_pmi) / (max_pmi - min_pmi)\n",
    "\n",
    "    return pmi_df\n",
    "\n",
    "\n",
    "def highlight_text(sentence, p_df,thres):\n",
    "    # Tokenize the sentence\n",
    "    words = word_tokenize(sentence)\n",
    "\n",
    "    # color words based on saliency scores from p_df\n",
    "    highlighted_sentence = \"\"\n",
    "    for word in words:\n",
    "        if word in p_df.columns:\n",
    "            # Get the saliency score for the word\n",
    "            saliency_score = p_df.loc['saliency', word]\n",
    "\n",
    "            # Set highlight color based on saliency score\n",
    "            if saliency_score > thres:\n",
    "                highlighted_word = f\"\\033[35m{word}\\033[0m\"  # Bold pink for high saliency\n",
    "            else:\n",
    "                highlighted_word = f\"\\033[91m{word}\\033[0m\"  # Purple for low saliency\n",
    "        else:\n",
    "            highlighted_word = word\n",
    "        highlighted_sentence += highlighted_word + \" \"\n",
    "\n",
    "    return highlighted_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          tokyo        is       the   capital      city            of  japan\n",
      "px          NaN  0.950000  0.900000  0.550000  0.900000  1.000000e+00   0.25\n",
      "py          NaN  0.950000  0.500000  0.350000  0.450000  1.000000e+00   0.25\n",
      "pxy         NaN  0.950000  0.500000  0.350000  0.450000  1.000000e+00   0.25\n",
      "pmi         NaN  0.074001  0.152003  0.862496  0.152003 -7.213476e-12   2.00\n",
      "saliency    NaN  0.037000  0.076002  0.431248  0.076002 -3.606738e-12   1.00\n",
      "\u001b[91mtokyo\u001b[0m \u001b[91mis\u001b[0m \u001b[91mthe\u001b[0m \u001b[35mcapital\u001b[0m \u001b[91mcity\u001b[0m \u001b[91mof\u001b[0m \u001b[35mjapan\u001b[0m \n"
     ]
    }
   ],
   "source": [
    "p_df = compute_pmi(sentence, all_responses, anchor_word_idx, prompts_per_word)\n",
    "print(p_df)\n",
    "\n",
    "# Highlight text based on saliency scores\n",
    "highlighted_sentence = highlight_text(sentence.lower(), p_df, thres=0.1)\n",
    "\n",
    "# Print highlighted text\n",
    "print(highlighted_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4. EXERCISE: Try more examples; maybe come up with your own. Report the results.\n",
    "\n",
    "* Try to come up with more examples and, change the anchor word/number of responses, and observe the results. What does the explanation mean? Do you think it's a nice explanation? Why and why not? \n",
    "* What's the limitation of the current method? When does the method fail to explain? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "Sentence: Photographer reveals wild desert life.\n",
    "\n",
    "In my first example, I kept the first index as the anchor word and decreased the number of prompts per word to 15. This decision was made considering the specificity of the sentence 'Photographer reveals wild desert life.' \n",
    "Given the niche nature of the subject matter, expecting the language model to produce meaningful replacements within such a constrained context could be overly ambitious. As we have a particular subject matter (wild desert life) and action (photographer reveals).\n",
    "Therefore, I narrowed down the scope abit to focus on generating relevant and coherent replacements. The sentence was inspired from BBC.news.com and shortened a bit to reduced running time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  Photographer reveals wild desert life\n"
     ]
    }
   ],
   "source": [
    " \n",
    "\n",
    "anchor_word_idx = 0 # the index of the interested word\n",
    "prompts_per_word = 15 # number of generated responses  \n",
    "\n",
    "sentence_1 = get_input()\n",
    "print(\"Sentence: \", sentence_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] [MASK] wild desert life:   7%|▋         | 1/15 [00:05<01:14,  5.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', '[mask]', 'wild', 'desert', 'life'] ['he', 'observed', 'the', 'diverse', 'wildlife', 'in', 'the', 'vast', 'desolate', 'desert']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] [MASK] wild desert life:  27%|██▋       | 4/15 [00:22<01:00,  5.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', '[mask]', 'wild', 'desert', 'life'] ['he', 'observed', 'the', 'fascinating', '[wild]', '[desert]', 'life']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] [MASK] wild desert life:  33%|███▎      | 5/15 [00:26<00:48,  4.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', '[mask]', 'wild', 'desert', 'life'] ['she', 'observed', 'the', 'diverse', 'wildlife', 'in', 'the', 'vast', 'desert', 'landscape']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] [MASK] wild desert life:  40%|████      | 6/15 [00:32<00:47,  5.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', '[mask]', 'wild', 'desert', 'life'] ['the', '[wild]', 'desert', 'life', 'thrives', 'on', 'the', '[mysterious]', 'dunes']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] [MASK] wild desert life:  47%|████▋     | 7/15 [00:35<00:37,  4.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', '[mask]', 'wild', 'desert', 'life'] ['giraffes', 'thrive', 'in', 'the', 'harsh', 'wild', 'desert', 'landscape']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] [MASK] wild desert life:  53%|█████▎    | 8/15 [00:39<00:30,  4.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', '[mask]', 'wild', 'desert', 'life'] ['elephants', 'thrive', 'in', 'the', 'vast', 'savannah']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] [MASK] wild desert life:  60%|██████    | 9/15 [00:46<00:30,  5.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', '[mask]', 'wild', 'desert', 'life'] ['she', 'observed', 'a', 'fascinating', 'variety', 'of', '[flora]', '[fauna]', 'in', 'the', 'wild', 'desert', 'landscape']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] [MASK] wild desert life:  67%|██████▋   | 10/15 [00:52<00:27,  5.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', '[mask]', 'wild', 'desert', 'life'] ['fierce', 'predators', 'thrive', 'in', 'this', 'vast', 'rugged', 'landscape']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] [MASK] wild desert life:  73%|███████▎  | 11/15 [00:55<00:19,  4.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', '[mask]', 'wild', 'desert', 'life'] ['foxes', 'thrive', 'in', 'the', 'vast', 'untamed', 'wilderness']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] [MASK] wild desert life:  80%|████████  | 12/15 [01:02<00:16,  5.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', '[mask]', 'wild', 'desert', 'life'] ['the', 'harsh', 'desert', 'life', 'is', 'teeming', 'with', 'various', 'species', 'such', 'as', 'coyotes', 'roadrunners', 'and', 'snakes']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] [MASK] wild desert life: 100%|██████████| 15/15 [01:18<00:00,  5.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', '[mask]', 'wild', 'desert', 'life'] ['the', 'harsh', 'desert', 'life', 'thrives', 'on', 'minimal', 'resources']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] reveals [MASK] desert life:  13%|█▎        | 2/15 [00:05<00:33,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'reveals', '[mask]', 'desert', 'life'] ['africa', 'unveils', 'exotic', 'desert', 'life']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] reveals [MASK] desert life:  20%|██        | 3/15 [00:09<00:37,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'reveals', '[mask]', 'desert', 'life'] ['the', 'fox', 'reveals', 'the', 'beauty', 'of', 'the', 'desert', 'landscape']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] reveals [MASK] desert life:  40%|████      | 6/15 [00:16<00:22,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'reveals', '[mask]', 'desert', 'life'] ['desert', 'reveals', 'arid', 'wildlife']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] reveals [MASK] desert life:  60%|██████    | 9/15 [00:25<00:18,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'reveals', '[mask]', 'desert', 'life'] ['the', 'fox', 'unveils', 'its', 'arid', 'desert', 'life']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] reveals [MASK] desert life:  73%|███████▎  | 11/15 [00:32<00:13,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'reveals', '[mask]', 'desert', 'life'] ['the', 'fox', 'unveils', 'fascinating', 'desert', 'life']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] reveals [MASK] desert life:  87%|████████▋ | 13/15 [00:40<00:07,  3.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'reveals', '[mask]', 'desert', 'life'] ['the', 'wildebeest', 'exposes', 'fascinating', 'desert', 'life']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] reveals [MASK] desert life: 100%|██████████| 15/15 [00:46<00:00,  3.13s/it]\n",
      "Input: [MASK] reveals wild [MASK] life:   7%|▋         | 1/15 [00:04<01:01,  4.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'reveals', 'wild', '[mask]', 'life'] ['mastiffs', 'reveal', 'wild', 'african', 'life']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] reveals wild [MASK] life:  13%|█▎        | 2/15 [00:07<00:44,  3.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'reveals', 'wild', '[mask]', 'life'] ['africa', 'unveils', 'wild', 'safari', 'life']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] reveals wild [MASK] life:  20%|██        | 3/15 [00:09<00:33,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'reveals', 'wild', '[mask]', 'life'] ['india', 'showcases', 'vibrant', 'rural', 'existence']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] reveals wild [MASK] life:  33%|███▎      | 5/15 [00:14<00:26,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'reveals', 'wild', '[mask]', 'life'] ['africa', 'unveils', 'thrilling', 'wildlife', 'experience']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] reveals wild [MASK] life:  40%|████      | 6/15 [00:17<00:24,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'reveals', 'wild', '[mask]', 'life'] ['the', 'sun', 'exposes', 'a', 'vivid', 'wildlife', 'scene']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] reveals wild [MASK] life:  47%|████▋     | 7/15 [00:20<00:23,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'reveals', 'wild', '[mask]', 'life'] ['the', 'author', 'unveils', 'a', 'thrilling', 'wildlife', 'documentary']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] reveals wild [MASK] life:  60%|██████    | 9/15 [00:31<00:25,  4.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'reveals', 'wild', '[mask]', 'life'] ['the', 'artist', 'unveils', 'a', 'vivid', 'exotic', 'lifestyle']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] reveals wild [MASK] life:  67%|██████▋   | 10/15 [00:34<00:19,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'reveals', 'wild', '[mask]', 'life'] ['africa', 'unveils', 'breathtaking', 'wildlife']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] reveals wild [MASK] life:  73%|███████▎  | 11/15 [00:38<00:14,  3.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'reveals', 'wild', '[mask]', 'life'] ['japan', 'unveils', 'mysterious', 'samurai', 'existence']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] reveals wild [MASK] life:  80%|████████  | 12/15 [00:41<00:10,  3.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'reveals', 'wild', '[mask]', 'life'] ['mars', 'unveils', 'its', 'mysterious', 'red', 'life']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] reveals wild [MASK] life:  87%|████████▋ | 13/15 [00:44<00:06,  3.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'reveals', 'wild', '[mask]', 'life'] ['mexico', 'unveils', 'vibrant', 'cityscape']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] reveals wild [MASK] life:  93%|█████████▎| 14/15 [00:49<00:03,  3.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'reveals', 'wild', '[mask]', 'life'] ['the', 'fox', 'unveils', 'its', 'extraordinary', 'life']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] reveals wild [MASK] life: 100%|██████████| 15/15 [00:52<00:00,  3.48s/it]\n",
      "Input: [MASK] reveals wild desert [MASK]:  33%|███▎      | 5/15 [00:25<01:02,  6.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'reveals', 'wild', 'desert', '[mask]'] ['the', 'scientist', 'unveils', 'wild', 'desert', 'landscapes']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] reveals wild desert [MASK]:  53%|█████▎    | 8/15 [00:33<00:25,  3.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'reveals', 'wild', 'desert', '[mask]'] ['the', 'coyote', 'unveils', 'a', 'stunning', 'oasis']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] reveals wild desert [MASK]:  67%|██████▋   | 10/15 [00:40<00:18,  3.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'reveals', 'wild', 'desert', '[mask]'] ['the', 'artist', 'unveils', 'a', 'breathtaking', 'painting', 'depicting', 'the', 'vast', 'expanse', 'of', 'a', 'wild', 'desert', 'landscape']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] reveals wild desert [MASK]:  73%|███████▎  | 11/15 [00:43<00:13,  3.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'reveals', 'wild', 'desert', '[mask]'] ['the', 'sun', 'sets', 'over', 'the', 'vast', 'expanse', 'of', 'wild', 'desert', 'terrain']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] reveals wild desert [MASK]:  87%|████████▋ | 13/15 [00:49<00:06,  3.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'reveals', 'wild', 'desert', '[mask]'] ['the', 'artist', 'unveils', 'a', 'stunning', 'painting', 'depicting', 'a', 'vast', 'and', 'barren', 'wilderness']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] reveals wild desert [MASK]:  93%|█████████▎| 14/15 [00:51<00:03,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'reveals', 'wild', 'desert', '[mask]'] ['the', 'artist', 'unveils', 'wild', 'desert', 'landscapes']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] reveals wild desert [MASK]: 100%|██████████| 15/15 [00:54<00:00,  3.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 \n",
      "                 \n",
      "                 \n",
      "                 \n",
      "                 \n",
      "[['', ''], ['fierce', 'predators thrive in the vast expanse of the untamed'], ['giraffes', 'thrive in the [african] [savannah]'], ['', ''], ['', ''], ['', ''], ['', ''], ['', ''], ['', ''], ['', ''], ['', ''], ['', ''], ['foxes', 'and rabbits experience challenging'], ['the', '[african] [sahara]'], ['', '']]\n",
      "[['he', 'fascinating'], ['', ''], ['', ''], ['the fox', 'its arid'], ['desert', 'harsh'], ['', ''], ['sheila', 'exotic'], ['he', 'stunning'], ['', ''], ['the eagle', 'breathtaking'], ['', ''], ['he', 'his'], ['', ''], ['the fox', 'the beauty of'], ['the fox', 'the harsh']]\n",
      "[['', ''], ['', ''], ['', ''], ['jeffrey', 'adventure'], ['', ''], ['', ''], ['', ''], ['the sentence should be science or nature', 'natural'], ['', ''], ['', ''], ['', ''], ['', ''], ['', ''], ['', ''], ['john', 'adventurous']]\n",
      "[['the coyote', 'landscape'], ['the camel', 'scenery'], ['the fox', 'canyons'], ['the coyote', 'desert landscape'], ['', ''], ['elon musk', 'landscapes'], ['the eagle', 'landscapes'], ['', ''], ['the coyote', 'scenery'], ['', ''], ['', ''], ['the camel', 'landscapes'], ['', ''], ['', ''], ['mystery', 'oasis']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_responses_1 = run_prompts(model, sentence_1, anchor_word_idx, prompts_per_word)\n",
    "print(\"                 \")\n",
    "print(\"                 \")\n",
    "print(\"                 \")\n",
    "print(\"                 \")\n",
    "print(\"                 \")\n",
    "\n",
    "for response in all_responses_1:\n",
    "    print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example from the mask out. \n",
    " \n",
    " \\[MASK-1\\] reveals \\[MASK-2\\] desert life.\n",
    "\n",
    "\n",
    "| MASK-1      | MASK-2 |\n",
    "| ----------- | ----------- |\n",
    "|    he   |     fascinating   |\n",
    "|  The fox  |     its arid   |\n",
    "|  Desert |     harsh    |\n",
    "|  Sheila  |     exotic    |\n",
    "|  he        |   stunning |\n",
    "|    the eagle   |     breathtaking   |\n",
    "|  he  |     his    |\n",
    "|  the fox  |     the beauty of   |\n",
    "|  the fox |     the harsh    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As anticipated, we observe that the model faced challenges in generating diverse and relevant alternatives given the specific context. It appears that the specificity of the prompt restricts the model's ability to explore various linguistic patterns. However, it does provide a comprehensive overview of what is typically associated with wild desert life. For instance, in some countries in Africa, the savannah is renowned as a place to experience wildlife. Additionally, the model suggests adjectives commonly used to describe such experiences. Therefore, the model in some way effectively aids in understanding the potential significance of a photographer revealing wild desert life."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          photographer       reveals          wild        desert          life\n",
      "px                 NaN  6.666667e-12  6.666667e-12  6.666667e-12  6.666667e-12\n",
      "py                 NaN  6.666667e-12  6.666667e-12  6.666667e-12  6.666667e-12\n",
      "pxy                NaN  6.666667e-12  6.666667e-12  6.666667e-12  6.666667e-12\n",
      "pmi                NaN  3.712617e+01  3.712617e+01  3.712617e+01  3.712617e+01\n",
      "saliency           NaN  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00\n",
      "\u001b[91mphotographer\u001b[0m \u001b[35mreveals\u001b[0m \u001b[35mwild\u001b[0m \u001b[35mdesert\u001b[0m \u001b[35mlife\u001b[0m \n"
     ]
    }
   ],
   "source": [
    "p_df_1 = compute_pmi(sentence_1, all_responses_1, anchor_word_idx, prompts_per_word)\n",
    "print(p_df_1)\n",
    "\n",
    "# Highlight text based on saliency scores\n",
    "highlighted_sentence = highlight_text(sentence_1.lower(), p_df_1, thres=0.1)\n",
    "\n",
    "# Print highlighted text\n",
    "print(highlighted_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aditionally, the consistent replacement of \"photographer\" with predominantly male nouns, pronouns, or even specific individuals like Elon Musk, as observed in the model's output, is interesting. This pattern suggests a potential bias or tendency within the language model towards associating certain roles or professions, such as photography, with masculinity. This bias could which may reflect societal stereotypes or gender norms present in the text corpora.\n",
    "\n",
    "The singular instance where a female name \"Sheila\" is suggested as a replacement for \"photographer,\" with the accompanying adjective \"exotic,\" is particularly noteworthy. This deviates from the predominantly male replacements and therefore does not follow the abovementioned pattern. However, it still raises questions about the factors influencing the model's decision-making process and why a feminine name like \"Sheila\" is associated with the adjective \"exotic\" in this context.\n",
    "\n",
    "Which led me to my next sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2\n",
    "Sentence: Young people are consumed by Instagram.\n",
    "\n",
    "In my second example, I set the second index as the anchor word so 'people' and decreased the number of prompts per word to 15. I wanted to test out a stereotypical sentence and chose this sentence to investigate if the replacement words would be dominated by 'women', 'females', 'girls'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  young people are consumed by instagram\n"
     ]
    }
   ],
   "source": [
    "anchor_word_idx = 1 # the index of the interested word\n",
    "prompts_per_word = 15 # number of generated responses  \n",
    "\n",
    "sentence_2 = get_input()\n",
    "print(\"Sentence: \", sentence_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] [MASK] are consumed by instagram: 100%|██████████| 15/15 [00:40<00:00,  2.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', '[mask]', 'are', 'consumed', 'by', 'instagram'] ['users', 'consume', 'instagram']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: young [MASK] [MASK] consumed by instagram:  47%|████▋     | 7/15 [00:20<00:23,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['young', '[mask]', '[mask]', 'consumed', 'by', 'instagram'] ['young', 'teenagers', 'obsessed', 'with', 'instagram']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: young [MASK] [MASK] consumed by instagram:  53%|█████▎    | 8/15 [00:22<00:18,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['young', '[mask]', '[mask]', 'consumed', 'by', 'instagram'] ['young', 'adults', 'engaged', 'in', 'social', 'media']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: young [MASK] [MASK] consumed by instagram:  87%|████████▋ | 13/15 [01:24<00:16,  8.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['young', '[mask]', '[mask]', 'consumed', 'by', 'instagram'] ['young', 'people', 'overwhelmed', 'by', 'instagram']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: young [MASK] [MASK] consumed by instagram: 100%|██████████| 15/15 [01:27<00:00,  5.85s/it]\n",
      "Input: young [MASK] are [MASK] by instagram: 100%|██████████| 15/15 [00:34<00:00,  2.30s/it]\n",
      "Input: young [MASK] are consumed [MASK] instagram:  60%|██████    | 9/15 [00:27<00:15,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['young', '[mask]', 'are', 'consumed', '[mask]', 'instagram'] ['young', 'teenagers', 'are', 'obsessed', 'with', 'instagram']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: young [MASK] are consumed [MASK] instagram: 100%|██████████| 15/15 [00:41<00:00,  2.77s/it]\n",
      "Input: young [MASK] are consumed by [MASK]: 100%|██████████| 15/15 [01:00<00:00,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 \n",
      "                 \n",
      "                 \n",
      "                 \n",
      "                 \n",
      "[['users', ''], ['photographers', 'and influencers'], ['people', ''], ['cats', 'and dogs'], ['people', 'nowadays'], ['young', 'people'], ['humans', 'and phones'], ['humans', 'and smartphones'], ['foodies', ''], ['cats', 'and dogs'], ['fruits', 'and vegetables'], ['users', ''], ['users', ''], ['people', ''], ['', '']]\n",
      "[['teenagers', 'are'], ['people', 'are'], ['couple', ''], ['teenagers', 'are'], ['female', 'student'], ['sentence', 'should be young people are often'], ['', ''], ['', ''], ['young', 'artist is constantly'], ['sentence', 'could be young teenagers are'], ['teenagers', ''], ['entrepreneur', ''], ['', ''], ['entrepreneurs', ''], ['woman', 'was']]\n",
      "[['teenagers', 'captivated'], ['athletes', 'inspired'], ['photographers', 'inspired'], ['people', 'inspired'], ['people', 'fascinated'], ['women', 'inundated'], ['students', 'captivated'], ['people', 'fascinated'], ['teenagers', 'captivated'], ['women', 'influenced'], ['people', 'addicted'], ['athletes', 'influenced'], ['adults', 'captivated'], ['people', 'influenced'], ['people', 'captivated']]\n",
      "[['people', 'by'], ['teenagers', 'by'], ['athletes', 'by'], ['people', 'by'], ['people', 'by'], ['people', 'by'], ['adults', 'by'], ['athletes', 'by'], ['', ''], ['people', 'by'], ['people', 'by'], ['teenagers', 'by'], ['people', 'by'], ['teenagers', 'by'], ['people', 'by']]\n",
      "[['turtles', 'predators'], ['children', 'curiosity'], ['whales', 'orcas'], ['children', 'curiosity'], ['koi', 'predators'], ['children', 'curiosity'], ['children', 'curiosity'], ['lions', 'hunger'], ['cicadas', 'ants'], ['dogs', 'hunger'], ['birds', 'predators'], ['boys', 'curiosity'], ['humans', 'emotions'], ['salmon', 'predators'], ['foxes', 'predators']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_responses_2 = run_prompts(model, sentence_2, anchor_word_idx, prompts_per_word)\n",
    "print(\"                 \")\n",
    "print(\"                 \")\n",
    "print(\"                 \")\n",
    "print(\"                 \")\n",
    "print(\"                 \")\n",
    "\n",
    "for response in all_responses_2:\n",
    "    print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 young  people       are      consumed            by  \\\n",
      "px        2.000000e-01     NaN  0.066667  4.000000e-01  5.333333e-01   \n",
      "py        6.666667e-12     NaN  0.200000  6.666667e-12  9.333333e-01   \n",
      "pxy       6.666667e-12     NaN  0.066667  6.666667e-12  5.333333e-01   \n",
      "pmi       2.321928e+00     NaN  2.321928  1.321928e+00  9.953567e-02   \n",
      "saliency  6.002145e-02     NaN  0.060021  3.301387e-02  1.096765e-12   \n",
      "\n",
      "             instagram  \n",
      "px        6.666667e-12  \n",
      "py        6.666667e-12  \n",
      "pxy       6.666667e-12  \n",
      "pmi       3.712617e+01  \n",
      "saliency  1.000000e+00  \n",
      "\u001b[91myoung\u001b[0m \u001b[91mpeople\u001b[0m \u001b[91mare\u001b[0m \u001b[91mconsumed\u001b[0m \u001b[91mby\u001b[0m \u001b[35minstagram\u001b[0m \n"
     ]
    }
   ],
   "source": [
    "p_df_2 = compute_pmi(sentence_2, all_responses_2, anchor_word_idx, prompts_per_word)\n",
    "print(p_df_2)\n",
    "\n",
    "# Highlight text based on saliency scores\n",
    "highlighted_sentence = highlight_text(sentence_2.lower(), p_df_2, thres=0.1)\n",
    "\n",
    "# Print highlighted text\n",
    "print(highlighted_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, the term \"Instagram\" emerges as significant within the context of the sentence, and upon masking it out, the entire domain shifts dramatically. The sentence transforms into an explanation of the natural order, where entities like young turtles or children are depicted as consumed by various forces such as predators or curiosity. These examples stand out as they deviate from the rest, illustrating how the specificity of \"Instagram\" restricts the range of potential interpretations beyond the realm of social media. Moreover, as anticipated, some replacements for \"young people\" include terms like \"woman\" or \"female,\" suggesting that the data underlying the model may have a preconceived notion of typical Instagram users, such as entrepreneurs, women, teenagers, and influencers. Notably, there is a lack of explicit male nouns in these replacements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Bonus Exercises\n",
    "### 5.1 Language pre-processing. \n",
    "In this exercise, we only lower the letters and split sentences into words; there's much more to do to pre-process the language. For example, contractions (*I'll*, *She's*, *world's*), suffix and prefix, compound words (*hard-working*). It's called word tokenization in NLP, and there are some Python packages that can do such work for us, e.g. [*TextBlob*](https://textblob.readthedocs.io/en/dev/). \n",
    "\n",
    "\n",
    "### 5.2 Better word matching\n",
    "In the above example of\n",
    "> Tokyo is the capital of Japan and a popular metropolis in the world.\n",
    "\n",
    ", GenAI never gives the specific word 'metropolis' when masking it out; instead, sometimes it provides words like 'city', which is not the same word but has a similar meaning. Instead of measuring the exact matching of certain words (i.e. 0 or 1), we can also measure the similarity of two words, e.g. the cosine similarity in word embedding, which ranges from 0 to 1. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
