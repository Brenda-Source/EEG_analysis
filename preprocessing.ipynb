{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "\n",
    "import mne\n",
    "from mne.datasets import eegbci\n",
    "from mne.io import concatenate_raws, read_raw_edf\n",
    "from mne.stats import permutation_cluster_1samp_test as pcluster_test\n",
    "from mne.time_frequency import tfr_multitaper, tfr_morlet\n",
    "from mne.time_frequency import morlet, fwhm\n",
    "import tables as tb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save in PyTables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the structure for the table\n",
    "class SubjectData(tb.IsDescription):\n",
    "    P_id = tb.StringCol(16)  # Participant ID\n",
    "    Averaged_Epochs = tb.Float64Col(shape=(4,))  # Averaged epochs\n",
    "    ICA_Components = tb.Float64Col(shape=(2,))  # ICA components\n",
    "    Alpha_Left_ERD = tb.Float64Col()  # Alpha left ERD\n",
    "    Alpha_Right_ERD = tb.Float64Col()  # Alpha right ERD\n",
    "    Beta_Left_ERD = tb.Float64Col()  # Beta left ERD\n",
    "    Beta_Right_ERD = tb.Float64Col()  # Beta right ERD\n",
    "\n",
    "# Create a new HDF5 file\n",
    "hdf5_file = tb.open_file(\"subject_data.h5\", mode=\"w\", title=\"Subject Data\")\n",
    "\n",
    "# Create a group for subjects\n",
    "subject_group = hdf5_file.create_group(\"/\", \"Subjects\", \"Subject Data\")\n",
    "\n",
    "# Create a table for each subject\n",
    "for subject_id in range(1, 51):\n",
    "    table = hdf5_file.create_table(subject_group, f\"Subject_{subject_id}\", SubjectData, f\"Subject {subject_id} Data\")\n",
    "\n",
    "# Close the HDF5 file\n",
    "hdf5_file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "%matplotlib qt\n",
    "\n",
    "def read_file(path):\n",
    "    r = mne.io.read_raw_bdf(path, preload=False)\n",
    "    \n",
    "    # Define signal info\n",
    "    n_time_samps = r.n_times\n",
    "    time_secs = r.times\n",
    "    ch_names = r.ch_names\n",
    "    n_chan = len(ch_names) \n",
    "    \n",
    "    \n",
    "    channels_to_keep = [\"A1\", \"A2\", \"A3\", \"A4\", \"A5\", \"A6\", \"A7\", \"A8\", \"A9\", \"A10\",\n",
    "            \"A11\", \"A12\", \"A13\", \"A14\", \"A15\", \"A16\", \"A17\", \"A18\", \"A19\", \"A20\",\n",
    "            \"A21\", \"A22\", \"A23\", \"A24\", \"A25\", \"A26\", \"A27\", \"A28\", \"A29\", \"A30\",\n",
    "            \"A31\", \"A32\", \"B1\", \"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\", \"B8\", \"B9\", \"B10\",\n",
    "            \"B11\", \"B12\", \"B13\", \"B14\", \"B15\", \"B16\", \"B17\", \"B18\", \"B19\", \"B20\",\n",
    "            \"B21\", \"B22\", \"B23\", \"B24\", \"B25\", \"B26\", \"B27\", \"B28\", \"B29\", \"B30\", \"B31\", \"B32\"]\n",
    "\n",
    "    channels_to_exclude_str = [ch for ch in r.ch_names if ch not in channels_to_keep + ['Status']]\n",
    "    \n",
    "    \n",
    "    raw = mne.io.read_raw_bdf(path, exclude = channels_to_exclude_str ,  preload=True)\n",
    "    \n",
    "    list_chan_name = { 'A3': 'AF3',\n",
    "     'A7': 'F7', 'A6': 'F5','A5': 'F3', 'A4': 'F1',\n",
    "     'A8': 'FT7', 'A9': 'FC5','A10': 'FC3', 'A11': 'FC1',\n",
    "     'A15': 'T7', 'A14': 'C5','A13': 'C3', 'A12': 'C1',\n",
    "     'A16': 'TP7', 'A17': 'CP5','A18': 'CP3', 'A19': 'CP1', 'A32': 'CPz',\n",
    "     'A24': 'P9', 'A23': 'P7','A22': 'P5', 'A21': 'P3', 'A20': 'P1','A31': 'Pz',\n",
    "     'A25': 'PO7', 'A26': 'PO3','A30': 'POz', \n",
    "     'A27': 'O1',   'A29': 'Oz', \n",
    "     'A28': 'Iz', \n",
    "     'B1': 'Fpz','B2': 'Fp2',\n",
    "     'B5': 'AFz', 'B4': 'AF4','B3': 'AF8',\n",
    "     'B6': 'Fz', 'B7': 'F2','B8': 'F4', 'B9': 'F6','B10': 'F8',\n",
    "     'B15': 'FCz', 'B14': 'FC2','B13': 'FC4', 'B12': 'FC6','B11': 'FT8',\n",
    "     'B16': 'Cz', 'B17': 'C2','B18': 'C4', 'B19': 'C6','B20': 'T8',\n",
    "     'B24': 'CP2', 'B23': 'CP4','B22': 'CP6', 'B21': 'TP8',\n",
    "     'B25': 'P2', 'B26': 'P4','B27': 'P6', 'B28': 'P8','B29': 'P10',\n",
    "     'B31': 'PO4', 'B30': 'PO8',\n",
    "     'B32': 'O2'\n",
    "     }\n",
    "\n",
    "    _ = raw.rename_channels(list_chan_name)\n",
    "    print(raw.info)\n",
    "    \n",
    "    return raw\n",
    "\n",
    "    \n",
    "def highlow_pass(raw):\n",
    "    # We cut off at one Hertz, because we will be looking at the alpha and beta frequency bands that start\n",
    "# from the 8-10 Hz. Apply high-pass filter\n",
    "    raw.filter(l_freq=1, h_freq=None, fir_design='firwin', filter_length='auto',\n",
    "           l_trans_bandwidth='auto', h_trans_bandwidth='auto', method='fir',\n",
    "           phase='zero', fir_window='hamming', verbose=True)\n",
    "\n",
    "# Apply low-pass filter if needed\n",
    "    raw.filter(l_freq=None, h_freq=40, fir_design='firwin', filter_length='auto',\n",
    "           l_trans_bandwidth='auto', h_trans_bandwidth='auto', method='fir',\n",
    "           phase='zero', fir_window='hamming', verbose=True)\n",
    "\n",
    "    return raw \n",
    "    \n",
    "    \n",
    "def plotsignal(signal, channels, pick_min, pick_max):\n",
    "    \n",
    "    ch_names = signal.ch_names\n",
    "    \n",
    "    \n",
    "    picks = ch_names[pick_min:pick_max]  # Pick the first 20 EEG channels\n",
    "    signal.plot(n_channels=channels, picks=picks, events = False, title='EEG Data for the Channels')\n",
    "    \n",
    "    \n",
    "   \n",
    "def rename_events(raw):\n",
    "    \n",
    "    # Find events and drop unnecessary\n",
    "    events_ID = mne.find_events(raw, stim_channel=\"Status\")\n",
    "    events = mne.pick_events(events_ID, exclude=[3, 5, 40, 42, 62])\n",
    "    \n",
    "    \n",
    "    unsuccessful_event_index = None  # For sanity check\n",
    "    \n",
    "    for i, event in enumerate(events):\n",
    "    \n",
    "        if event[2] == 10:\n",
    "            \n",
    "            next_event_index = i + 1    \n",
    "            while next_event_index < len(events) and events[next_event_index][2] == 61: \n",
    "                events[next_event_index][2] = 610 # Modify the trigger value\n",
    "                next_event_index += 1   \n",
    "        \n",
    "        if event[2] == 20:\n",
    "            next_event_index = i + 1    \n",
    "            while next_event_index < len(events) and events[next_event_index][2] == 61: \n",
    "                events[next_event_index][2] = 620 # Modify the trigger value\n",
    "                next_event_index += 1   \n",
    "        \n",
    "        \n",
    "        if event[2] == 30:\n",
    "            next_event_index = i + 1    \n",
    "            while next_event_index < len(events) and events[next_event_index][2] == 61: \n",
    "                events[next_event_index][2] = 630 # Modify the trigger value\n",
    "                next_event_index += 1   \n",
    "\n",
    "        else:\n",
    "            unsuccessful_event_index = i\n",
    "\n",
    "    # Check if any events with trigger value 61 are still present\n",
    "    if any(event[2] == 61 for event in events):\n",
    "        print(\"Renaming was unsuccessful :(\")\n",
    "        if unsuccessful_event_index is not None:\n",
    "            print(\"Problematic event index:\", unsuccessful_event_index)\n",
    "            print(\"Problematic event details:\", events[unsuccessful_event_index])\n",
    "   \n",
    "    # Define epoch parameters\n",
    "    tmin = -2\n",
    "    tmax = 3\n",
    "\n",
    "    \n",
    "    \n",
    "    # Rename events with numerical IDs to have corresponding event names\n",
    "    event_dict = {\"10\": 10, \n",
    "                  \"20\": 20,\n",
    "                  \"30\": 30,\n",
    "                  \"selftap\": 41, \n",
    "                  \"VP610\": 610,\n",
    "                  \"VP620\": 620, \n",
    "                  \"VP630\": 630}\n",
    "    \n",
    "    # Epoch signal\n",
    "    epochs = mne.Epochs(raw, events, event_id = event_dict, tmin=tmin, tmax=tmax, baseline=None)\n",
    "    \n",
    " \n",
    "\n",
    "    # Might introduce edge artifacts for each epoch.\n",
    "    epochs.load_data()\n",
    "    epochs_rs = epochs.resample(sfreq=256)\n",
    "\n",
    "    # Print events\n",
    "    event_ids = epochs_rs.events[:, -1]\n",
    "    print()\n",
    "    unique_event_ids = set(event_ids)\n",
    "    print(\"Unique Event IDs:\", unique_event_ids)\n",
    "    # Print counts of each event ID\n",
    "    for event_id in unique_event_ids:\n",
    "        count = (event_ids == event_id).sum()\n",
    "        print(\"Event ID:\", event_id, \"Count:\", count)\n",
    "\n",
    "    return epochs\n",
    "\n",
    "def drop_bad_interp(epochs, channels_list):\n",
    "    \n",
    "    ep_ds = epochs.copy()\n",
    "\n",
    "    # Highlight bad channels # we keep the flat one as it will be fixed after averaging \n",
    "    _ = ep_ds.info[\"bads\"].extend(channels_list)  \n",
    "\n",
    "    # Set montage and interpolate bads \n",
    "    ep_ds.set_montage('standard_1020')\n",
    "    ep_interp = ep_ds.interpolate_bads(\n",
    "    reset_bads=False, method='MNE', verbose=True\n",
    "    )\n",
    "    \n",
    "    return ep_interp\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def delete_variables_except(exceptions):\n",
    "    \"\"\"\n",
    "    Delete all variables in the global namespace except for the specified exceptions.\n",
    "    \n",
    "    Parameters:\n",
    "    exceptions (list): List of variable names to exclude from deletion.\n",
    "    \"\"\"\n",
    "    # Get a list of all variable names in the global namespace\n",
    "    all_variables = list(globals().keys())\n",
    "    \n",
    "    # Iterate through all variables and delete those not in the exceptions list\n",
    "    for var_name in all_variables:\n",
    "        if var_name not in exceptions:\n",
    "            # Check if the object is a variable (not a module, function, etc.)\n",
    "            if not isinstance(globals()[var_name], (types.FunctionType, types.ModuleType)):\n",
    "                del globals()[var_name]\n",
    "    \n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_epochs(sub,data_dir,epochs,name):\n",
    "    # saves Epoch\n",
    "    epochs.save(data_dir + sub + name + '-epo.fif', overwrite=True)\n",
    "    \n",
    "    \n",
    "def save_ica(subject,directionary,ica):\n",
    "        \n",
    "        ica.save(directionary + subject+ '-ica.fif', \n",
    "        overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ICA and waveanalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_ica(epochs, sub):\n",
    "    \n",
    "    # run fast ICA \n",
    "    ica = mne.preprocessing.ICA(\n",
    "    n_components=30, method=\"fastica\", max_iter=\"auto\", random_state=97)\n",
    "    \n",
    "    ica.fit(epochs) # fit on epochs\n",
    "    \n",
    "    # Visualise ICA components\n",
    "    montage = mne.channels.make_standard_montage(\"standard_1005\") \n",
    "    epochs.set_montage(montage)\n",
    "\n",
    "    ica.plot_sources(epochs) # plot the raw signal \n",
    "\n",
    "    #scalp field topographies \n",
    "    ica.plot_components() # plot a Topomap and Chooose ICA compoenents.\n",
    "    data_dir = 'C:/Users/Mabel Ife/OneDrive - Danmarks Tekniske Universitet/Dokumenter/Master in AI/Final Thesis/Data/Ica_files/'\n",
    "    \n",
    "    \n",
    "\n",
    "    save_ica(sub,data_dir,ica)\n",
    "    \n",
    "    return ica\n",
    "    \n",
    "\n",
    "def get_ica_sources(epochs, list_of_Ica_components, ica):\n",
    "    \n",
    "    \n",
    "    all_sources = ica.get_sources(epochs)\n",
    "\n",
    "    \n",
    "    # Get the sources corresponding to the components\n",
    "    motor_source = all_sources.pick(list_of_Ica_components)\n",
    "    \n",
    "    # separate conditions\n",
    "    motor_source_event41 = motor_source['selftap']\n",
    "    motor_source_event610 = motor_source['VP610']\n",
    "    motor_source_event620 = motor_source['VP620']\n",
    "    motor_source_event630 = motor_source['VP630']\n",
    "    \n",
    "    \n",
    "    # combine all conditions to one list \n",
    "    all_motor_components = [motor_source_event41, motor_source_event610, motor_source_event620, motor_source_event630]\n",
    "    \n",
    "    return all_motor_components\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def basecorrection_convert_to_df(p):\n",
    "    \n",
    "    # Define baseline period\n",
    "    baseline_tmin = -0.5\n",
    "    baseline_tmax = 1.5\n",
    "\n",
    "    time_interval = [baseline_tmin, baseline_tmax]\n",
    "    \n",
    "    interval_baseline = [i for i in p.times if i >= time_interval[0] and i <= time_interval[1]]\n",
    "    baseline = []\n",
    "    \n",
    "    # Find the indices corresponding to the time interval\n",
    "    start_index = np.where(p.times >= time_interval[0])[0][0]\n",
    "    end_index = np.where(p.times <= time_interval[1])[0][-1]\n",
    "\n",
    "    \n",
    "    # Calculate baseline\n",
    "    for e in range(len(p.data)):\n",
    "        avg_epoch = np.mean(p.data[e, :, :, start_index:end_index], axis=-1)\n",
    "        baseline.append(avg_epoch)\n",
    "    baseline = np.asarray(baseline)\n",
    "\n",
    "    \n",
    "    print(\"Shape of p.data:\", p.data.shape)\n",
    "    print(\"Shape of baseline:\", baseline.shape)\n",
    "    \n",
    "    baseline_reshaped = np.expand_dims(baseline, axis=-1) \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Subtract baseline and normalize\n",
    "    for e in range(len(p.data)):\n",
    "        for i in range(len(p.ch_names)):\n",
    "            p.data[e, i, :] -= baseline_reshaped[e, i]  # Subtract baseline\n",
    "            p.data[e, i, :] /= baseline_reshaped[e, i]  # Normalize\n",
    "\n",
    "    \n",
    "    return p\n",
    "      \n",
    "\n",
    "def wave_analysis(motor_source):\n",
    "\n",
    "\n",
    "    # Perform wavelet analysis\n",
    "    freqs = np.arange(1, 31) # interested in frequencies 1 to 30 hz\n",
    "    n_cycles = 3 + 0.125 * freqs # Number of cycles   \n",
    "                                \n",
    "    p = mne.time_frequency.tfr_morlet(motor_source, freqs, n_cycles, use_fft=False, return_itc=False,\n",
    "\n",
    "                        decim=1, n_jobs=None, zero_mean=True, average=False, output='power',\n",
    "\n",
    "                        picks=\"all\")\n",
    "         \n",
    "         \n",
    "    # Output p datandarray, shape (n_epochs, n_channels, n_freqs, n_times)\n",
    "    \n",
    "    return  p\n",
    "\n",
    "\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get ERD's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_alpha_beta_comps(Basecorrect_epochs):\n",
    "    \n",
    "    \n",
    "    # Define frequency bands\n",
    "    alpha_band = [8, 12]\n",
    "    beta_band = [13, 25]\n",
    "\n",
    "    # Define time window\n",
    "    tmin = -0.5\n",
    "    tmax = 1.5\n",
    "    baseline = (tmin,tmax)\n",
    "\n",
    "    \n",
    "    cropped_epochs = Basecorrect_epochs.copy().crop(tmin, tmax)\n",
    "    #cropped_epochs  = Basecorrect_epochs.apply_baseline(baseline)\n",
    "\n",
    "    \n",
    "    p_alpha_left = cropped_epochs.copy().crop(fmin = alpha_band[0], fmax = alpha_band[1]).pick(picks = ['ICA009']).average(dim = 'freqs')\n",
    "    p_beta_left = cropped_epochs.copy().crop(fmin = beta_band[0], fmax = beta_band[1]).pick(picks = ['ICA009']).average(dim = 'freqs')\n",
    "    p_alpha_right = cropped_epochs.copy().crop(fmin = alpha_band[0], fmax = alpha_band[1]).pick(picks = ['ICA008']).average(dim = 'freqs')\n",
    "    p_beta_right = cropped_epochs.copy().crop(fmin = beta_band[0], fmax = beta_band[1]).pick(picks = ['ICA008']).average(dim = 'freqs')\n",
    "\n",
    "    \n",
    "    \n",
    "    print(\"Shape of epochs data averaged across alpha band for left component:\", p_alpha_left.data.shape)\n",
    "    print(\"Timepoints for above mentioned data:\", len(p_alpha_left.times))\n",
    "    \n",
    "    # Compute ERD &  # datandarray, shape (n_channels, n_freqs, n_times)\n",
    "    erd_alpha_left = p_alpha_left.average(dim = 'epochs')\n",
    "    erd_beta_left = p_beta_left.average(dim = 'epochs')\n",
    "    erd_alpha_right = p_alpha_right.average(dim = 'epochs')\n",
    "    erd_beta_right = p_beta_right.average(dim = 'epochs')\n",
    "    \n",
    "    print(\" \")\n",
    "    print(\"Shape of epochs data averaged across trials for left component:\", erd_alpha_left.data.shape)\n",
    "    \n",
    "    return erd_alpha_left, erd_alpha_right, erd_beta_left, erd_beta_right\n",
    "    #return p_alpha_left, p_alpha_right, p_beta_left, p_beta_right\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from C:\\Users\\Mabel Ife\\OneDrive - Danmarks Tekniske Universitet\\Dokumenter\\Master in AI\\Final Thesis\\Data\\subj_07_070223_task.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from C:\\Users\\Mabel Ife\\OneDrive - Danmarks Tekniske Universitet\\Dokumenter\\Master in AI\\Final Thesis\\Data\\subj_07_070223_task.bdf...\n",
      "BDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 4517887  =      0.000 ...  2206.000 secs...\n",
      "<Info | 8 non-empty values\n",
      " bads: []\n",
      " ch_names: A1, A2, AF3, F1, F3, F5, F7, FT7, FC5, FC3, FC1, C1, C3, C5, T7, ...\n",
      " chs: 64 EEG, 1 Stimulus\n",
      " custom_ref_applied: False\n",
      " highpass: 0.0 Hz\n",
      " lowpass: 417.0 Hz\n",
      " meas_date: 2023-02-07 14:46:06 UTC\n",
      " nchan: 65\n",
      " projs: []\n",
      " sfreq: 2048.0 Hz\n",
      " subject_info: 1 item (dict)\n",
      ">\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up high-pass filter at 1 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal highpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Filter length: 6759 samples (3.300 s)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    1.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  64 out of  64 | elapsed:   15.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up low-pass filter at 40 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal lowpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Upper passband edge: 40.00 Hz\n",
      "- Upper transition bandwidth: 10.00 Hz (-6 dB cutoff frequency: 45.00 Hz)\n",
      "- Filter length: 677 samples (0.331 s)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  64 out of  64 | elapsed:   12.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigger channel Status has a non-zero initial value of {initial_value} (consider using initial_event=True to detect this event)\n",
      "Removing orphaned offset at the beginning of the file.\n",
      "4237 events found on stim channel Status\n",
      "Event IDs: [ 3  5 10 20 30 40 41 42 61 62]\n",
      "Not setting metadata\n",
      "1707 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 1707 events and 10241 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "Unique Event IDs: {610, 41, 10, 620, 20, 630, 30}\n",
      "Event ID: 610 Count: 450\n",
      "Event ID: 41 Count: 540\n",
      "Event ID: 10 Count: 30\n",
      "Event ID: 620 Count: 301\n",
      "Event ID: 20 Count: 30\n",
      "Event ID: 630 Count: 326\n",
      "Event ID: 30 Count: 30\n"
     ]
    }
   ],
   "source": [
    "# Loads data, filters raw signal, artefact rejection, average reference\n",
    "sample_fname = \"C:/Users/Mabel Ife/OneDrive - Danmarks Tekniske Universitet/Dokumenter/Master in AI/Final Thesis/Data/subj_07_070223_task.bdf\"\n",
    "raw_filt = highlow_pass(read_file(sample_fname))  # reads file and performs high and low pass filtering\n",
    "epochs_rs = rename_events(raw_filt)# renames events, create epochs, resample to 256HZ\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting channel interpolation method to {'eeg': 'MNE'}.\n",
      "Interpolating bad channels.\n",
      "    Automatic origin fit: head of radius 95.6 mm\n",
      "    Computing dot products for 63 EEG channels...\n",
      "    Computing cross products for 63 → 64 EEG channels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Preparing the mapping matrix...\n",
      "    Truncating at 63/63 components and regularizing with α=1.0e-01\n",
      "EEG channel type selected for re-referencing\n",
      "Applying average reference.\n",
      "Applying a custom ('EEG',) reference.\n",
      "Overwriting existing file.\n",
      "Overwriting existing file.\n",
      "Overwriting existing file.\n",
      "Overwriting existing file.\n",
      "Overwriting existing file.\n",
      "Overwriting existing file.\n",
      "Overwriting existing file.\n",
      "Overwriting existing file.\n"
     ]
    }
   ],
   "source": [
    "ep_interp = drop_bad_interp(epochs_rs,['TP8']) # artefact rejection and interpolation of channels\n",
    "\n",
    "# use the average of all channels as reference\n",
    "ep_avg_ref = ep_interp.set_eeg_reference(ref_channels=\"average\")\n",
    "\n",
    "# Save epochs in events.\n",
    "epochs_trial41 = ep_avg_ref['selftap']\n",
    "epochs_trial610 = ep_avg_ref['VP610']\n",
    "epochs_trial620 = ep_avg_ref['VP620']\n",
    "epochs_trial630 = ep_avg_ref['VP630']\n",
    "\n",
    "\n",
    "p_id = 'sub-007'\n",
    "data_dir = 'C:/Users/Mabel Ife/OneDrive - Danmarks Tekniske Universitet/Dokumenter/Master in AI/Final Thesis/Data/epochs/'\n",
    "\n",
    "save_epochs(p_id,data_dir,epochs_trial41, '-AVtrial41')\n",
    "save_epochs(p_id,data_dir,epochs_trial610, '-AVtrial610')\n",
    "save_epochs(p_id,data_dir,epochs_trial620, '-AVtrial620')\n",
    "save_epochs(p_id,data_dir,epochs_trial630, '-AVtrial630')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the subject 7, we found the two components 8 and 9. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ICA and Wavelet analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free-up memory\n",
    "delete_variables_except(['ep_avg_ref'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting ICA to data using 63 channels (please be patient, this may take a while)\n",
      "Selecting by number: 30 components\n",
      "Fitting ICA took 265.8s.\n",
      "Not setting metadata\n",
      "1707 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using matplotlib as 2D backend.\n",
      "Writing ICA solution to C:\\Users\\Mabel Ife\\OneDrive - Danmarks Tekniske Universitet\\Dokumenter\\Master in AI\\Final Thesis\\Data\\Ica_files\\sub-007-ica.fif...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    4.4s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    8.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    8.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of p.data: (540, 2, 30, 1280)\n",
      "Shape of baseline: (540, 2, 30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    3.7s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    7.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    7.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of p.data: (450, 2, 30, 1280)\n",
      "Shape of baseline: (450, 2, 30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.4s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    4.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    4.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of p.data: (301, 2, 30, 1280)\n",
      "Shape of baseline: (301, 2, 30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    5.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    5.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of p.data: (326, 2, 30, 1280)\n",
      "Shape of baseline: (326, 2, 30)\n"
     ]
    }
   ],
   "source": [
    "# fit ICA and look for motor components\n",
    "p_id = 'sub-007'\n",
    "ICA = fit_ica(ep_avg_ref,p_id)\n",
    "\n",
    "# Pick components (comment out when needed)\n",
    "# ICA.plot_properties(ep_avg_ref, picks=[8,9], log_scale=False)\n",
    "\n",
    "\n",
    "# get sources for components\n",
    "all_conditions = get_ica_sources(ep_avg_ref,['ICA008','ICA009'],ICA)\n",
    "\n",
    "# Run wavelet and basecorrection \n",
    "\n",
    "baseCorr_power = [] # saves baselinecorrected epochs \n",
    "\n",
    "for motor_source_condition in all_conditions: # loop through each condition\n",
    "    p_df = basecorrection_convert_to_df(wave_analysis(motor_source_condition))\n",
    "    \n",
    "    baseCorr_power.append(p_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of epochs data averaged across alpha band for left component: (540, 1, 1, 513)\n",
      "Timepoints for above mentioned data: 513\n",
      " \n",
      "Shape of epochs data averaged across trials for left component: (1, 1, 513)\n",
      "Shape of epochs data averaged across alpha band for left component: (450, 1, 1, 513)\n",
      "Timepoints for above mentioned data: 513\n",
      " \n",
      "Shape of epochs data averaged across trials for left component: (1, 1, 513)\n",
      "Shape of epochs data averaged across alpha band for left component: (301, 1, 1, 513)\n",
      "Timepoints for above mentioned data: 513\n",
      " \n",
      "Shape of epochs data averaged across trials for left component: (1, 1, 513)\n",
      "Shape of epochs data averaged across alpha band for left component: (326, 1, 1, 513)\n",
      "Timepoints for above mentioned data: 513\n",
      " \n",
      "Shape of epochs data averaged across trials for left component: (1, 1, 513)\n"
     ]
    }
   ],
   "source": [
    "# Get ERD's in alfa and beta frenquencies for each component\n",
    "erd_alpha_L41, erd_alpha_R41, erd_beta_L41, erd_beta_R41 = Get_alpha_beta_comps(baseCorr_power[0])\n",
    "erd_alpha_L610, erd_alpha_610, erd_beta_610, erd_beta_610 = Get_alpha_beta_comps(baseCorr_power[1])\n",
    "erd_alpha_L620, erd_alpha_620, erd_beta_620, erd_beta_620 = Get_alpha_beta_comps(baseCorr_power[2])\n",
    "erd_alpha_L630, erd_alpha_630, erd_beta_630, erd_beta_630 = Get_alpha_beta_comps(baseCorr_power[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot ERD's \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "\n",
    "# A\n",
    "time_points = np.linspace(-0.5, 512 / 256, 513)  # 512 samples with 256 Hz sampling rate\n",
    "\n",
    "\n",
    "ax.set_title('Plot1:ICA_left, alpha')\n",
    "ax.set_xlabel('Time Instances')\n",
    "ax.set_ylabel('Volt')\n",
    "\n",
    "ax.plot(time_points, erd_alpha_L41.data[0,0,:], color='blue', label='self Tap')\n",
    "ax.plot(time_points, erd_alpha_L610.data[0,0,:], color='red', label='Non-adaptive')\n",
    "ax.plot(time_points, erd_alpha_L620.data[0,0,:], color='green', label='Moderately adaptive')\n",
    "ax.plot(time_points, erd_alpha_L630.data[0,0,:], color='pink', label='Overly adaptive')\n",
    "\n",
    "# Adding vertical lines at specified time points\n",
    "ax.axvline(x=0, color='black', linestyle='--', linewidth=1)  # at time point 0\n",
    "ax.axvline(x=1, color='black', linestyle='--', linewidth=1)  # at time point 1\n",
    "ax.axvline(x=1.5, color='black', linestyle='--', linewidth=1)  # at time point 1.5\n",
    "\n",
    "legend = ax.legend(loc='upper right', shadow=True, fontsize='medium')\n",
    "plt.title('ERDs of different conditions for left ICA component in alpha freq-range')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alpha_beta_comps(basecorrect_epochs):\n",
    "    \n",
    "    \n",
    "    # Define frequency bands\n",
    "    alpha_band = [8, 12]\n",
    "    beta_band = [13, 25]\n",
    "\n",
    "    # Define time window\n",
    "    tmin = -0.5\n",
    "    tmax = 1.5\n",
    "\n",
    "    # Select components \n",
    "    left_component = 1\n",
    "    right_component = 0\n",
    "\n",
    "    # Extract relevant data, apply filters, and select components\n",
    "    p_alpha_left = basecorrect_epochs.copy().crop(tmin, tmax,alpha_band[0], alpha_band[1]).pick(picks = ['ICA009']).average()\n",
    "    p_beta_left = basecorrect_epochs.copy().crop(tmin, tmax,alpha_band[0], alpha_band[1]).pick(picks = ['ICA009']).average()\n",
    "    p_alpha_right = basecorrect_epochs.copy().crop(tmin, tmax,alpha_band[0], alpha_band[1]).pick(picks = ['ICA008']).average()\n",
    "    p_beta_right = basecorrect_epochs.copy().crop(tmin, tmax,alpha_band[0], alpha_band[1]).pick(picks = ['ICA008']).average()\n",
    "\n",
    "    # Compute ERD\n",
    "    erd_alpha_left = np.mean(p_alpha_left.data, axis=0)\n",
    "    erd_beta_left = np.mean(p_beta_left.data, axis=0)\n",
    "    erd_alpha_right = np.mean(p_alpha_right.data, axis=0)\n",
    "    erd_beta_right = np.mean(p_beta_right.data, axis=0)\n",
    "    \n",
    "    \n",
    "    return erd_alpha_left, erd_alpha_right, erd_beta_left, erd_beta_right\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
