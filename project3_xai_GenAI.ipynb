{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda activate xai_llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  Tokyo is the capital city of Japan\n"
     ]
    }
   ],
   "source": [
    "def get_input():\n",
    "    # ideally this reads inputs from a file, now it just takes an input\n",
    "    return input(\"Enter a sentence: \")\n",
    "    \n",
    "anchor_word_idx = 0 # the index of the interested word\n",
    "prompts_per_word = 20 # number of generated responses  \n",
    "\n",
    "sentence = get_input()\n",
    "print(\"Sentence: \", sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from ./models/openchat-3.5-0106.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = openchat_openchat-3.5-0106\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32002]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32002]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32002]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 261/32002 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32002\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = openchat_openchat-3.5-0106\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 32000 '<|end_of_turn|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4165.38 MiB\n",
      "................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 160\n",
      "llama_new_context_with_model: n_batch    = 150\n",
      "llama_new_context_with_model: n_ubatch   = 150\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    20.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   20.00 MiB, K (f16):   10.00 MiB, V (f16):   10.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    23.53 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: openchat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.name': 'openchat_openchat-3.5-0106', 'general.architecture': 'llama', 'llama.context_length': '8192', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '32000', 'general.file_type': '15', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{{ 'GPT4 Correct ' + message['role'].title() + ': ' + message['content'] + '<|end_of_turn|>'}}{% endfor %}{% if add_generation_prompt %}{{ 'GPT4 Correct Assistant:' }}{% endif %}\"}\n",
      "Using gguf chat template: {{ bos_token }}{% for message in messages %}{{ 'GPT4 Correct ' + message['role'].title() + ': ' + message['content'] + '<|end_of_turn|>'}}{% endfor %}{% if add_generation_prompt %}{{ 'GPT4 Correct Assistant:' }}{% endif %}\n",
      "Using chat eos_token: <|end_of_turn|>\n",
      "Using chat bos_token: <s>\n"
     ]
    }
   ],
   "source": [
    "from models.ChatModel import ChatModel\n",
    "model_name = \"openchat\"\n",
    "model = ChatModel(model_name)\n",
    "print(f\"Model: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] [MASK] the capital city of Japan:  75%|███████▌  | 15/20 [00:29<00:06,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', '[mask]', 'the', 'capital', 'city', 'of', 'japan'] ['tokyo']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] [MASK] the capital city of Japan: 100%|██████████| 20/20 [00:37<00:00,  1.88s/it]\n",
      "Input: [MASK] is [MASK] capital city of Japan:  10%|█         | 2/20 [00:04<00:37,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'is', '[mask]', 'capital', 'city', 'of', 'japan'] ['tokyo', 'is', 'japans', 'capital', 'city']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] is [MASK] capital city of Japan:  20%|██        | 4/20 [00:07<00:29,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'is', '[mask]', 'capital', 'city', 'of', 'japan'] ['tokyo', 'is', 'japans', 'capital', 'city']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] is [MASK] capital city of Japan: 100%|██████████| 20/20 [00:36<00:00,  1.83s/it]\n",
      "Input: [MASK] is the [MASK] city of Japan:  20%|██        | 4/20 [00:09<00:38,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'is', 'the', '[mask]', 'city', 'of', 'japan'] ['tokyo', 'is', 'the', 'bustling', 'metropolis', 'of', 'japan']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] is the [MASK] city of Japan:  75%|███████▌  | 15/20 [00:31<00:10,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'is', 'the', '[mask]', 'city', 'of', 'japan'] ['tokyo', 'is', 'the', 'bustling', 'metropolis', 'of', 'japan']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] is the [MASK] city of Japan:  80%|████████  | 16/20 [00:33<00:08,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'is', 'the', '[mask]', 'city', 'of', 'japan'] ['tokyo', 'is', 'the', 'vibrant', 'metropolis', 'of', 'japan']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] is the [MASK] city of Japan: 100%|██████████| 20/20 [00:41<00:00,  2.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'is', 'the', '[mask]', 'city', 'of', 'japan'] ['tokyo', 'is', 'the', 'bustling', 'metropolis', 'of', 'japan']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] is the capital [MASK] of Japan: 100%|██████████| 20/20 [00:34<00:00,  1.71s/it]\n",
      "Input: [MASK] is the capital city [MASK] Japan: 100%|██████████| 20/20 [00:34<00:00,  1.73s/it]\n",
      "Input: [MASK] is the capital city of [MASK]: 100%|██████████| 20/20 [00:35<00:00,  1.79s/it]\n"
     ]
    }
   ],
   "source": [
    "# Get responses\n",
    "\n",
    "from tools.command_generator import generate_prompts, prefix_prompt\n",
    "from tools.evaluate_response import get_replacements\n",
    "from tqdm import tqdm\n",
    "\n",
    "def run_prompts(model, sentence, anchor_idx, prompts_per_word=20):\n",
    "    prompts = generate_prompts(sentence, anchor_idx)\n",
    "    all_replacements = []\n",
    "    for prompt in prompts:\n",
    "        replacements = []\n",
    "        for _ in tqdm(\n",
    "            range(prompts_per_word),\n",
    "            desc=f\"Input: {prompt}\",\n",
    "        ):\n",
    "            response = model.get_response(\n",
    "                prefix_prompt(prompt),\n",
    "            ).strip()\n",
    "            if response:\n",
    "                replacement = get_replacements(prompt, response)\n",
    "                if replacement:\n",
    "                    replacements.append(replacement)\n",
    "        if len(replacements) > 0:\n",
    "            all_replacements.append(replacements)\n",
    "    return all_replacements\n",
    "\n",
    "all_responses = run_prompts(model, sentence, anchor_word_idx, prompts_per_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['tokyo', 'is'], ['tokyo', 'is'], ['tokyo', 'is'], ['tokyo', 'is'], ['tokyo', 'is'], ['tokyo', 'is'], ['tokyo', 'is'], ['tokyo', 'is'], ['tokyo', 'is'], ['tokyo', 'is'], ['tokyo', 'is'], ['tokyo', 'is'], ['tokyo', 'is'], ['tokyo', 'is'], ['', ''], ['tokyo', 'is'], ['tokyo', 'is'], ['tokyo', 'is'], ['tokyo', 'is'], ['tokyo', 'is']]\n",
      "[['tokyo', 'the'], ['', ''], ['tokyo', 'the'], ['', ''], ['tokyo', 'tokyos'], ['tokyo', '[mask]'], ['tokyo', 'tokyos'], ['tokyo', 'the'], ['tokyo', 'the'], ['tokyo', 'tokyos'], ['tokyo', 'the'], ['tokyo', 'tokyos'], ['tokyo', 'tokyos'], ['tokyo', 'the'], ['tokyo', 'the'], ['tokyo', 'the'], ['tokyo', 'the'], ['tokyo', 'the'], ['tokyo', 'the'], ['tokyo', '[tokyo]']]\n",
      "[['tokyo', 'largest'], ['osaka', 'second largest'], ['tokyo', 'capital'], ['', ''], ['tokyo', 'bustling capital'], ['kyoto', 'ancient'], ['tokyo', 'capital'], ['osaka', 'second largest'], ['tokyo', 'capital'], ['kyoto', 'ancient capital'], ['tokyo', 'capital'], ['tokyo', 'capital'], ['tokyo', 'capital'], ['osaka', 'secondlargest'], ['', ''], ['', ''], ['kyoto', 'ancient'], ['tokyo', 'vibrant capital'], ['tokyo', 'capital'], ['', '']]\n",
      "[['tokyo', ''], ['tokyo', 'city'], ['tokyo', 'city'], ['tokyo', ''], ['tokyo', 'city'], ['tokyo', '[tōkyō]'], ['tokyo', 'city'], ['tokyo', ''], ['tokyo', 'city'], ['tokyo', 'city'], ['tokyo', 'city'], ['tokyo', ''], ['tokyo', 'city'], ['tokyo', ''], ['tokyo', ''], ['tokyo', 'city'], ['tokyo', 'tokyo'], ['tokyo', 'city'], ['tokyo', ''], ['tokyo', 'city']]\n",
      "[['tokyo', 'of'], ['tokyo', 'of'], ['tokyo', 'of'], ['tokyo', 'of'], ['tokyo', 'of'], ['tokyo', 'of'], ['tokyo', 'of'], ['tokyo', 'of'], ['tokyo', 'of'], ['tokyo', 'of'], ['tokyo', 'of'], ['tokyo', 'of'], ['tokyo', 'of'], ['tokyo', 'of'], ['tokyo', 'of'], ['tokyo', 'of'], ['tokyo', 'of'], ['tokyo', 'of'], ['tokyo', 'of'], ['tokyo', 'of']]\n",
      "[['paris', 'france'], ['london', 'the united kingdom'], ['paris', 'france'], ['london', 'united kingdom'], ['paris', 'france'], ['paris', 'france'], ['london', 'the united kingdom'], ['paris', 'france'], ['paris', 'france'], ['paris', 'france'], ['new delhi', 'india'], ['paris', 'france'], ['paris', 'france'], ['paris', 'france'], ['paris', 'france'], ['paris', 'france'], ['paris', 'france'], ['tokyo', 'japan'], ['paris', 'france'], ['london', 'the united kingdom']]\n"
     ]
    }
   ],
   "source": [
    "for response in all_responses:\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\s184075\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s184075\\AppData\\Local\\Temp\\ipykernel_5600\\3802064522.py:91: RuntimeWarning: divide by zero encountered in log2\n",
      "  pmi = np.log2(co_occurrence_count / ((freq_dist[words[anchor_word_idx]] + epsilon) * (freq_dist[word] + epsilon)))\n",
      "C:\\Users\\s184075\\AppData\\Local\\Temp\\ipykernel_5600\\3802064522.py:100: RuntimeWarning: invalid value encountered in scalar subtract\n",
      "  pmi_scores[word] = (pmi_scores[word] - pmi_min) / (pmi_max - pmi_min)\n",
      "C:\\Users\\s184075\\AppData\\Local\\Temp\\ipykernel_5600\\3802064522.py:100: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  pmi_scores[word] = (pmi_scores[word] - pmi_min) / (pmi_max - pmi_min)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argument of type 'NoneType' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 121>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    118\u001b[0m pmi_scores \u001b[38;5;241m=\u001b[39m compute_pmi1(sentence,all_responses, anchor_word_idx)\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m# Highlight text based on PMI scores\u001b[39;00m\n\u001b[1;32m--> 121\u001b[0m highlighted_sentence \u001b[38;5;241m=\u001b[39m \u001b[43mhighlight_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpmi_scores\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36mhighlight_text\u001b[1;34m(sentence, pmi_scores)\u001b[0m\n\u001b[0;32m     36\u001b[0m highlighted_sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words:\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mword\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpmi_scores\u001b[49m:\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;66;03m# Set highlight color based on PMI score\u001b[39;00m\n\u001b[0;32m     40\u001b[0m         pmi_score \u001b[38;5;241m=\u001b[39m pmi_scores[word]\n\u001b[0;32m     41\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m pmi_score \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.7\u001b[39m:\n",
      "\u001b[1;31mTypeError\u001b[0m: argument of type 'NoneType' is not iterable"
     ]
    }
   ],
   "source": [
    "def compute_pmi(sentence, all_responses, anchor_word_idx):\n",
    "    # Tokenizing\n",
    "    words = word_tokenize(sentence.lower())\n",
    "    # Combine all responses into a single list of tokens\n",
    "    all_tokens = [word for response in all_responses for word in word_tokenize(str(response).lower())]\n",
    "\n",
    "    freq_dist = FreqDist(all_tokens)\n",
    "\n",
    "   \n",
    "    # Compute PMI for each word\n",
    "    pmi_scores = {}\n",
    "    for i, word in enumerate(words):\n",
    "        if i != anchor_word_idx:\n",
    "            # Compute co-occurrence counts\n",
    "            co_occurrence_count = freq_dist[word]\n",
    "\n",
    "            # Avoid division by zero by adding a small epsilon value\n",
    "            epsilon = 1e-10\n",
    "\n",
    "            # Compute PMI\n",
    "            pmi = np.log2(co_occurrence_count / ((freq_dist[words[anchor_word_idx]] + epsilon) * (freq_dist[word] + epsilon)))\n",
    "            \n",
    "            # Min-Max normalization\n",
    "            pmi_normalized = (pmi - min(pmi_scores.values())) / (max(pmi_scores.values()) - min(pmi_scores.values()))\n",
    "            \n",
    "            pmi_scores[word] = pmi_normalized\n",
    "\n",
    "    return pmi_scores\n",
    "\n",
    "\n",
    "def highlight_text(sentence, pmi_scores):\n",
    "    # Tokenize the sentence\n",
    "    words = word_tokenize(sentence)\n",
    "\n",
    "    # Highlight words based on PMI scores\n",
    "    highlighted_sentence = \"\"\n",
    "    for word in words:\n",
    "        if word in pmi_scores:\n",
    "            # Set highlight color based on PMI score\n",
    "            pmi_score = pmi_scores[word]\n",
    "            if pmi_score > 0.7:\n",
    "                highlighted_word = f\"\\033[33m{word}\\033[0m\"  # Bold pink for positive PMI\n",
    "            elif pmi_score > 0.4:\n",
    "                highlighted_word = f\"\\033[34m{word}\\033[0m\" \n",
    "            else:\n",
    "                highlighted_word = f\"\\033[35m{word}\\033[0m\"  # Pink for negative PMI\n",
    "        else:\n",
    "            highlighted_word = word\n",
    "        highlighted_sentence += highlighted_word + \" \"\n",
    "\n",
    "    return highlighted_sentence\n",
    "\n",
    "def highlight_text2(sentence, scores):\n",
    "    words = word_tokenize(sentence)\n",
    "    highlighted_sentence = \"\"\n",
    "    for i, word in enumerate(words):\n",
    "        if word in scores:\n",
    "            score = scores[word]\n",
    "            if score > 0.5:\n",
    "                highlighted_word = colored(word, 'orange')\n",
    "            elif score > 0.3:\n",
    "                highlighted_word = colored(word, 'blue')\n",
    "            else:\n",
    "                highlighted_word = colored(word, 'green')  # Using green as the third color\n",
    "        else:\n",
    "            highlighted_word = word\n",
    "        highlighted_sentence += highlighted_word + \" \"\n",
    "    return highlighted_sentence\n",
    "\n",
    "import pandas as pd\n",
    "from termcolor import colored\n",
    "def compute_pmi1(sentence, all_responses, anchor_word_idx):\n",
    "    # Tokenizing\n",
    "    words = word_tokenize(sentence.lower())\n",
    "    # Combine all responses into a single list of tokens\n",
    "    all_tokens = [word for response in all_responses for word in word_tokenize(str(response).lower())]\n",
    "\n",
    "    freq_dist = FreqDist(all_tokens)\n",
    "\n",
    "    # Compute PMI for each word\n",
    "    pmi_scores = {}\n",
    "    for i, word in enumerate(words):\n",
    "        if i != anchor_word_idx:\n",
    "            # Compute co-occurrence counts\n",
    "            co_occurrence_count = freq_dist[word]\n",
    "\n",
    "            # Avoid division by zero by adding a small epsilon value\n",
    "            epsilon = 1e-10\n",
    "\n",
    "            # Compute PMI\n",
    "            pmi = np.log2(co_occurrence_count / ((freq_dist[words[anchor_word_idx]] + epsilon) * (freq_dist[word] + epsilon)))\n",
    "            \n",
    "            pmi_scores[word] = pmi\n",
    "\n",
    "    # Min-Max normalization\n",
    "    if pmi_scores:\n",
    "        pmi_min = min(pmi_scores.values())\n",
    "        pmi_max = max(pmi_scores.values())\n",
    "        for word in pmi_scores:\n",
    "            pmi_scores[word] = (pmi_scores[word] - pmi_min) / (pmi_max - pmi_min)\n",
    "\n",
    "def pmi_scores_to_dataframe(pmi_scores):\n",
    "    # Convert PMI scores dictionary to DataFrame\n",
    "    df = pd.DataFrame.from_dict(pmi_scores, orient='index', columns=['PMI'])\n",
    "    df.index.name = 'Word'\n",
    "    return df\n",
    "\n",
    "def display_pmi_scores(pmi_scores):\n",
    "    # Convert PMI scores to DataFrame\n",
    "    df = pmi_scores_to_dataframe(pmi_scores)\n",
    "    \n",
    "    # Display DataFrame\n",
    "    print(\"PMI Scores between Words:\")\n",
    "    print(df)\n",
    "\n",
    "\n",
    "# Compute PMI\n",
    "pmi_scores = compute_pmi1(sentence,all_responses, anchor_word_idx)\n",
    "\n",
    "# Highlight text based on PMI scores\n",
    "highlighted_sentence = highlight_text(sentence.lower(), pmi_scores)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokyo is the capital city of japan \n",
      "PMI Scores between Words:\n",
      "               PMI\n",
      "Word              \n",
      "is            -inf\n",
      "the           -inf\n",
      "capital  33.219281\n",
      "city          -inf\n",
      "of            -inf\n",
      "japan         -inf\n"
     ]
    }
   ],
   "source": [
    "# Print highlighted text\n",
    "print(highlighted_sentence)\n",
    "display_pmi_scores(pmi_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_pmi(sentence, anchor_word_idx):\n",
    "    # Tokenize the sentence\n",
    "    words = word_tokenize(sentence.lower())\n",
    "\n",
    "    # Remove punctuation tokens\n",
    "    words = [word for word in words if word.isalnum()]\n",
    "\n",
    "    # Compute frequency distribution of tokens\n",
    "    freq_dist = FreqDist(words)\n",
    "\n",
    "    # Compute PMI for each word\n",
    "    pmi_scores = {}\n",
    "    for i, word in enumerate(words):\n",
    "        if i != anchor_word_idx:\n",
    "            # Compute co-occurrence counts\n",
    "            co_occurrence_count = freq_dist[word]\n",
    "\n",
    "            # Compute PMI\n",
    "            pmi = np.log2(co_occurrence_count / (freq_dist[words[anchor_word_idx]] * freq_dist[word]))\n",
    "            pmi_scores[word] = pmi\n",
    "\n",
    "    return pmi_scores\n",
    "\n",
    "def highlight_text(sentence, pmi_scores):\n",
    "    # Tokenize the sentence\n",
    "    words = word_tokenize(sentence)\n",
    "\n",
    "    # Highlight words based on PMI scores\n",
    "    highlighted_sentence = \"\"\n",
    "    for word in words:\n",
    "        if word in pmi_scores:\n",
    "            # Set highlight color based on PMI score\n",
    "            pmi_score = pmi_scores[word]\n",
    "            if pmi_score > 0:\n",
    "                highlighted_word = f\"\\033[91m{word}\\033[0m\"  # Red for positive PMI\n",
    "            else:\n",
    "                highlighted_word = f\"\\033[92m{word}\\033[0m\"  # Green for negative PMI\n",
    "        else:\n",
    "            highlighted_word = word\n",
    "        highlighted_sentence += highlighted_word + \" \"\n",
    "\n",
    "    return highlighted_sentence\n",
    "\n",
    "\n",
    "\n",
    "# Compute PMI\n",
    "pmi_scores = compute_pmi(sentence, anchor_word_idx)\n",
    "\n",
    "# Highlight text based on PMI scores\n",
    "highlighted_sentence = highlight_text(sentence.lower(), pmi_scores)\n",
    "\n",
    "# Print highlighted text\n",
    "print(highlighted_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pmi(sentence, all_responses, anchor_word_idx, prompts_per_word):\n",
    "    # Tokenize the sentence (splits )\n",
    "    words = word_tokenize(sentence.lower())\n",
    "\n",
    "    # Clean punctuation and so forth \n",
    "    tokens = [token for token in words if token.isalnum()]\n",
    "    \n",
    "    # Convert words to lowercase\n",
    "    words = [word.lower() for word in tokens]\n",
    "\n",
    "    # Create a DataFrame to store PMI calculations\n",
    "    pmi_df = pd.DataFrame(columns=words)\n",
    "    pmi_df.loc['px'] = 0\n",
    "    pmi_df.loc['py'] = 0\n",
    "    pmi_df.loc['pxy'] = 0\n",
    "    pmi_df.loc['pmi'] = 0\n",
    "    pmi_df.loc['saliency'] = 0\n",
    "\n",
    "    # variables\n",
    "    idx_y = 0\n",
    "    word_x = words[anchor_word_idx].lower()\n",
    "\n",
    "    epsi = 1e-10 # no division by zero \n",
    "    # Iterate over each response\n",
    "    for i, responses in enumerate(all_responses):\n",
    "        px = epsi\n",
    "        py = epsi\n",
    "        pxy = epsi\n",
    "\n",
    "        # Check response index matches the anchor word index\n",
    "        if anchor_word_idx == i:\n",
    "            idx_y = 1\n",
    "\n",
    "        word_y = words[i + idx_y].lower()\n",
    "\n",
    "        # Iterate over each response pair\n",
    "        for response in responses:\n",
    "            x = response[0]\n",
    "            y = response[1]\n",
    "            if x == word_x:\n",
    "                px += 1\n",
    "            if y == word_y:\n",
    "                py += 1\n",
    "            if x == word_x and y == word_y:\n",
    "                pxy += 1\n",
    "\n",
    "        # Calculate probabilities and PMI\n",
    "        px = px / prompts_per_word\n",
    "        py = py / prompts_per_word\n",
    "        pxy = pxy / prompts_per_word\n",
    "        pmi = np.log2(pxy / (px * py))\n",
    "\n",
    "        # Update DataFrame with calculated values\n",
    "        pmi_df.at['px', word_y] = px\n",
    "        pmi_df.at['py', word_y] = py\n",
    "        pmi_df.at['pxy', word_y] = pxy\n",
    "        pmi_df.at['pmi', word_y] = pmi\n",
    "\n",
    "    # Set the anchor word column to NaN\n",
    "    pmi_df[word_x] = np.nan\n",
    "\n",
    "    # Normalize PMI values and calculate saliency\n",
    "    min_pmi = np.round(pmi_df.loc['pmi'].min(), 10)\n",
    "    max_pmi = pmi_df.loc['pmi'].max()\n",
    "    pmi_df.loc['saliency'] = (pmi_df.loc['pmi'] - min_pmi) / (max_pmi - min_pmi)\n",
    "\n",
    "    return pmi_df\n",
    "\n",
    "\n",
    "def highlight_text(sentence, p_df):\n",
    "    # Tokenize the sentence\n",
    "    words = word_tokenize(sentence)\n",
    "\n",
    "    # Highlight words based on saliency scores from p_df\n",
    "    highlighted_sentence = \"\"\n",
    "    for word in words:\n",
    "        if word in p_df.columns:\n",
    "            # Get the saliency score for the word\n",
    "            saliency_score = p_df.loc['saliency', word]\n",
    "\n",
    "            # Set highlight color based on saliency score\n",
    "            if saliency_score > 0.1:\n",
    "                highlighted_word = f\"\\033[35m{word}\\033[0m\"  # Bold pink for high saliency\n",
    "            else:\n",
    "                highlighted_word = f\"\\033[91m{word}\\033[0m\"  # Purple for low saliency\n",
    "        else:\n",
    "            highlighted_word = word\n",
    "        highlighted_sentence += highlighted_word + \" \"\n",
    "\n",
    "    return highlighted_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          tokyo        is       the   capital          city            of  \\\n",
      "px          NaN  0.950000  0.900000  0.500000  1.000000e+00  1.000000e+00   \n",
      "py          NaN  0.950000  0.550000  0.350000  5.500000e-01  1.000000e+00   \n",
      "pxy         NaN  0.950000  0.550000  0.350000  5.500000e-01  1.000000e+00   \n",
      "pmi         NaN  0.074001  0.152003  1.000000 -7.213476e-12 -7.213476e-12   \n",
      "saliency    NaN  0.017122  0.035170  0.231378 -1.669041e-12 -1.669041e-12   \n",
      "\n",
      "             japan  \n",
      "px        0.050000  \n",
      "py        0.050000  \n",
      "pxy       0.050000  \n",
      "pmi       4.321928  \n",
      "saliency  1.000000  \n",
      "\u001b[91mtokyo\u001b[0m \u001b[91mis\u001b[0m \u001b[91mthe\u001b[0m \u001b[35mcapital\u001b[0m \u001b[91mcity\u001b[0m \u001b[91mof\u001b[0m \u001b[35mjapan\u001b[0m \n"
     ]
    }
   ],
   "source": [
    "p_df = compute_pmi(sentence, all_responses, anchor_word_idx, prompts_per_word)\n",
    "print(p_df)\n",
    "\n",
    "# Highlight text based on saliency scores\n",
    "highlighted_sentence = highlight_text(sentence.lower(), p_df)\n",
    "\n",
    "# Print highlighted text\n",
    "print(highlighted_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
